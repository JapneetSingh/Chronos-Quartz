{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from urllib2 import urlopen\n",
    "import urllib\n",
    "import requests\n",
    "from pymongo import MongoClient\n",
    "import json#for storing beautiful soup to mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_db = client['images_final']\n",
    "image_data = img_db['image_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To get a beautiful soup object\n",
    "def make_soup(url):\n",
    "    \n",
    "    #html = urlopen(url).read()\n",
    "    page = requests.get(url)\n",
    "    html = page.text\n",
    "    return BeautifulSoup(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns the product specification table FROM amazon product page\n",
    "def get_prod_table(soup):\n",
    "    tab = soup.select('table.a-keyvalue.a-spacing-mini')[0] # Because it returns 1 table in a list\n",
    "    #print tab\n",
    "    table = {} \n",
    "    for tr in tab.findAll('tr'):\n",
    "        try: \n",
    "            table[str(tr.th.get_text().strip())]= str((tr.td.get_text()).strip())\n",
    "        except:\n",
    "            #return \"Not available\"#No exception raised here since not needed. We can move to next element of table\n",
    "            pass #will ensure that this image and its metadata are not returned moving onto the next link\n",
    "    return table   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns the product description FROM amazon product page\n",
    "def get_prod_desc(soup):\n",
    "    try :\n",
    "        desc = soup.findAll('div',attrs={'id' : 'productDescription'})[0]#all product description are kept like this\n",
    "        return desc.get_text().strip()\n",
    "    except:\n",
    "        return \"Not available\" #Returns not available when no description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns the title of the amazon product page\n",
    "def get_prod_title(soup):\n",
    "    try:\n",
    "        title = soup.findAll('span',attrs={'id' : 'productTitle'})[0]\n",
    "        return str(title.get_text().strip())  \n",
    "    except:\n",
    "        return \"Not available\" #Returns not available when no title(generaly never the case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Insert data into mongodb\n",
    "\n",
    "def insert_to_mongo(soup,prod_url,img_url,link_counter):\n",
    "    \n",
    "    #Get decription table and product description\n",
    "    table = get_prod_table(soup)\n",
    "    \n",
    "    description = get_prod_desc(soup)\n",
    "    title = get_prod_title(soup)\n",
    "    try:\n",
    "        image_data.insert_one({ 'img_no' : link_counter,'prod_url':prod_url,'img_url': img_url,\"prod_info_table\":table,\"prod_desc\": description, \"title\" :title  })\n",
    "    except:\n",
    "        print \"MongoDb Insertion Error %d\"%link_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a link and the number of link creates a folder using number and stores images in it   \n",
    "#Link counter counts each page images are scraped from used to save the images\n",
    "def get_images(prod_url, link_counter):  \n",
    "    \n",
    "    #Make soup with product url\n",
    "    soup = make_soup(prod_url)\n",
    "    \n",
    "    #this makes a list of bs4 element image tags\n",
    "    images = [img for img in soup.select('img.a-dynamic-image.a-stretch-vertical')]\n",
    "    \n",
    "    #print (str(len(images)) + \"images found.\")\n",
    "    #print 'Downloading images to current working directory.'\n",
    "    #compile our unicode list of image links\n",
    "    \n",
    "    image_links = [each['data-old-hires'] for each in images]\n",
    "    \n",
    "    #print image_links\n",
    "    #To standardize the name of each image on prod_url we use file_name as a counter\n",
    "    file_name = 0\n",
    "    \n",
    "    #Create a sub directory for each link and go to that directory\n",
    "    os.mkdir('{0}'.format(link_counter))\n",
    "    os.chdir('{0}'.format(link_counter)) #Decided to keep all data in one directory\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Get images and store them \n",
    "    for each in image_links:\n",
    "        try:\n",
    "            filename='{0}.jpg'.format(link_counter)\n",
    "            #print \"???????????????\", filename\n",
    "            #print each\n",
    "            insert_to_mongo(soup,prod_url,each,link_counter)\n",
    "            \n",
    "            urllib.urlretrieve(each, filename)\n",
    "            #Alternative to urlretrieve\n",
    "            #r = requests.get(each)\n",
    "            #with open(filename,'wb') as f:\n",
    "            #f.write(r.content)\n",
    "            file_name  += 1\n",
    "            #insert text to mongodb\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "            #print \"Image Insertion Error %d\"%link_counter #Insertion error will automatically be raised from insert_to_mongo\n",
    "        \n",
    "    # return to original directory\n",
    "    os.chdir('..')\n",
    "    return image_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def lets_scrape_amazon(init_url,no_of_pages, prog_track_interval = 10):\n",
    "\n",
    "    link_counter  = 0\n",
    "\n",
    "    for i in range(1,(no_of_pages+1)):\n",
    "        # Link for wristwatches with rating > 4 stars\n",
    "        url =init_url.format(i,i)\n",
    "        \n",
    "        #Make soup with the search page\n",
    "        soup = make_soup(url)\n",
    "        \n",
    "        #Get the individual links to the product for each search page\n",
    "        links = [link.get('href') for link in soup.select(\"a.a-link-normal.s-access-detail-page\")]\n",
    "        \n",
    "        for link in links:\n",
    "            get_images(link,link_counter)\n",
    "            #Printing to track progress\n",
    "            if (link_counter% prog_track_interval) == 0 :\n",
    "                print \"%d images downloaded\"%(link_counter)\n",
    "            link_counter += 1 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "watch_search_url = \"http://www.amazon.com/s/ref=sr_pg_{0}?fst=as%3Aoff&rh=n%3A7141123011%2Cn%3A10445813011%2Cn%3A7147441011%2Cn%3A6358539011%2Cn%3A6358540011%2Ck%3Awrist+watches%2Cp_72%3A2661618011&page={1}&bbn=10445813011&keywords=wrist+watches&ie=UTF8&qid=1451726453\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images downloaded\n",
      "100 images downloaded\n",
      "200 images downloaded\n",
      "300 images downloaded\n",
      "400 images downloaded\n",
      "500 images downloaded\n",
      "600 images downloaded\n",
      "700 images downloaded\n",
      "800 images downloaded\n",
      "900 images downloaded\n",
      "1000 images downloaded\n",
      "1100 images downloaded\n",
      "1200 images downloaded\n",
      "1300 images downloaded\n",
      "1400 images downloaded\n",
      "1500 images downloaded\n",
      "1600 images downloaded\n",
      "1700 images downloaded\n",
      "1800 images downloaded\n",
      "1900 images downloaded\n",
      "2000 images downloaded\n",
      "2100 images downloaded\n",
      "2200 images downloaded\n",
      "2300 images downloaded\n",
      "2400 images downloaded\n",
      "2500 images downloaded\n",
      "2600 images downloaded\n",
      "2700 images downloaded\n",
      "2800 images downloaded\n",
      "2900 images downloaded\n",
      "3000 images downloaded\n",
      "3100 images downloaded\n",
      "3200 images downloaded\n",
      "3300 images downloaded\n",
      "3400 images downloaded\n",
      "3500 images downloaded\n",
      "3600 images downloaded\n",
      "3700 images downloaded\n",
      "3800 images downloaded\n",
      "3900 images downloaded\n",
      "4000 images downloaded\n",
      "4100 images downloaded\n",
      "4200 images downloaded\n",
      "4300 images downloaded\n",
      "4400 images downloaded\n",
      "4500 images downloaded\n",
      "4600 images downloaded\n",
      "4700 images downloaded\n",
      "4800 images downloaded\n",
      "4900 images downloaded\n",
      "5000 images downloaded\n",
      "5100 images downloaded\n",
      "5200 images downloaded\n",
      "5300 images downloaded\n",
      "5400 images downloaded\n",
      "5500 images downloaded\n",
      "5600 images downloaded\n",
      "5700 images downloaded\n",
      "5800 images downloaded\n",
      "5900 images downloaded\n"
     ]
    }
   ],
   "source": [
    "lets_scrape_amazon(watch_search_url,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Iskandar/Desktop/WatchSeer/Data2\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
